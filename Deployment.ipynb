{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.11.0 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080', major=8, minor=6, total_memory=10239MB, multi_processor_count=68)\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models import resnet\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Gen_dataset(images_path,  threads, mean, std, batch_size=1):\n",
    "    dataset = datasets.ImageFolder(root=images_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_path = './data/images/Images'\n",
    "batch_size = 1\n",
    "threads = 18\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset =  Gen_dataset(images_path, batch_size, threads, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/Testing/terrier.jpg\"\n",
    "image = Image.open(file_path)\n",
    "transform = transforms.Compose([\n",
    "                                         transforms.Resize((224,224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(torch.Tensor(mean),\n",
    "                                                              torch.Tensor(std))])\n",
    "image = transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence\n",
    "\n",
    "#from torchvision.models.utils import load_state_dict_from_url\n",
    "from torchvision.models.mobilenetv2 import _make_divisible, ConvBNActivation\n",
    "\n",
    "\n",
    "__all__ = [\"MobileNetV3\", \"mobilenet_v3_large\", \"mobilenet_v3_small\"]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"mobilenet_v3_large\": \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\",\n",
    "    \"mobilenet_v3_small\": \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: Tensor, inplace: bool) -> Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return F.hardsigmoid(scale, inplace=inplace)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "\n",
    "\n",
    "class InvertedResidualConfig:\n",
    "\n",
    "    def __init__(self, input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool,\n",
    "                 activation: str, stride: int, dilation: int, width_mult: float):\n",
    "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        self.kernel = kernel\n",
    "        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n",
    "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        self.use_se = use_se\n",
    "        self.use_hs = activation == \"HS\"\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_mult: float):\n",
    "        return _make_divisible(channels * width_mult, 8)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "\n",
    "    def __init__(self, cnf: InvertedResidualConfig, norm_layer: Callable[..., nn.Module],\n",
    "                 se_layer: Callable[..., nn.Module] = SqueezeExcitation):\n",
    "        super().__init__()\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError('illegal stride value')\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.Hardswish if cnf.use_hs else nn.ReLU\n",
    "\n",
    "        # expand\n",
    "        if cnf.expanded_channels != cnf.input_channels:\n",
    "            layers.append(ConvBNActivation(cnf.input_channels, cnf.expanded_channels, kernel_size=1,\n",
    "                                           norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "\n",
    "        # depthwise\n",
    "        stride = 1 if cnf.dilation > 1 else cnf.stride\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.expanded_channels, kernel_size=cnf.kernel,\n",
    "                                       stride=stride, dilation=cnf.dilation, groups=cnf.expanded_channels,\n",
    "                                       norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "        if cnf.use_se:\n",
    "            layers.append(se_layer(cnf.expanded_channels))\n",
    "\n",
    "        # project\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Identity))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.out_channels = cnf.out_channels\n",
    "        self._is_cn = cnf.stride > 1\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result += input\n",
    "        return result\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            inverted_residual_setting: List[InvertedResidualConfig],\n",
    "            last_channel: int,\n",
    "            num_classes: int = 120,\n",
    "            block: Optional[Callable[..., nn.Module]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        MobileNet V3 main class\n",
    "\n",
    "        Args:\n",
    "            inverted_residual_setting (List[InvertedResidualConfig]): Network structure\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "            num_classes (int): Number of classes\n",
    "            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not inverted_residual_setting:\n",
    "            raise ValueError(\"The inverted_residual_setting should not be empty\")\n",
    "        elif not (isinstance(inverted_residual_setting, Sequence) and\n",
    "                  all([isinstance(s, InvertedResidualConfig) for s in inverted_residual_setting])):\n",
    "            raise TypeError(\"The inverted_residual_setting should be List[InvertedResidualConfig]\")\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        # building first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(ConvBNActivation(3, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Hardswish))\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for cnf in inverted_residual_setting:\n",
    "            layers.append(block(cnf, norm_layer))\n",
    "\n",
    "        # building last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = 6 * lastconv_input_channels\n",
    "        layers.append(ConvBNActivation(lastconv_input_channels, lastconv_output_channels, kernel_size=1,\n",
    "                                       norm_layer=norm_layer, activation_layer=nn.Hardswish))\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lastconv_output_channels, last_channel),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(last_channel, num_classes),\n",
    "            #added by me\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _mobilenet_v3_conf(arch: str, params: Dict[str, Any]):\n",
    "    # non-public config parameters\n",
    "    reduce_divider = 2 if params.pop('_reduced_tail', False) else 1\n",
    "    dilation = 2 if params.pop('_dilated', False) else 1\n",
    "    width_mult = params.pop('_width_mult', 1.0)\n",
    "\n",
    "    bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n",
    "    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_mult=width_mult)\n",
    "\n",
    "    if arch == \"mobilenet_v3_large\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, False, \"RE\", 1, 1),\n",
    "            bneck_conf(16, 3, 64, 24, False, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(24, 3, 72, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 72, 40, True, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 3, 240, 80, False, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(80, 3, 200, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 480, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 3, 672, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 5, 672, 160 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1280 // reduce_divider)  # C5\n",
    "    elif arch == \"mobilenet_v3_small\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 288, 96 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1024 // reduce_divider)  # C5\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type {}\".format(arch))\n",
    "\n",
    "    return inverted_residual_setting, last_channel\n",
    "\n",
    "\n",
    "def _mobilenet_v3_model(\n",
    "    arch: str,\n",
    "    inverted_residual_setting: List[InvertedResidualConfig],\n",
    "    last_channel: int,\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    "):\n",
    "    model = MobileNetV3(inverted_residual_setting, last_channel, **kwargs)\n",
    "    if pretrained:\n",
    "        if model_urls.get(arch, None) is None:\n",
    "            raise ValueError(\"No checkpoint is available for model type {}\".format(arch))\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_large(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a large MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_large\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_v3_small(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a small MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_small\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dstok\\anaconda3\\envs\\ML\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py:25: FutureWarning: The ConvBNReLU/ConvBNActivation classes are deprecated since 0.12 and will be removed in 0.14. Use torchvision.ops.misc.ConvNormActivation instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mobilenet_v3_large(pretrained=False).to(device)\n",
    "model.load_state_dict(torch.load(\"./models/80%ModelLarge.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = dataset.classes\n",
    "class_names = [classes for classes in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irish_terrier\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "image = image.to(device)\n",
    "# print(img.shape)\n",
    "output = model(torch.unsqueeze(image, 0))\n",
    "breed = class_names[torch.argmax(output)]\n",
    "breed = breed[breed.index(\"-\") + 1: len(breed)]\n",
    "print(breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:Torch version 1.11.0 has not been tested with coremltools. You may run into unexpected errors. Torch 1.10.2 is the most recent version that has been tested.\n",
      "Converting Frontend ==> MIL Ops: 100%|█████████▉| 466/467 [00:00<00:00, 1862.36 ops/s]\n",
      "Running MIL Common passes:   0%|          | 0/34 [00:00<?, ? passes/s]C:\\Users\\dstok\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\converters\\mil\\mil\\passes\\name_sanitization_utils.py:101: UserWarning: Input, 'x.1', of the source model, has been renamed to 'x_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "C:\\Users\\dstok\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\converters\\mil\\mil\\passes\\name_sanitization_utils.py:129: UserWarning: Output, '930', of the source model, has been renamed to 'var_930' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████| 34/34 [00:00<00:00, 125.35 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 9/9 [00:00<00:00, 138.34 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 667/667 [00:00<00:00, 2708.99 ops/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to load libmodelpackage. Cannot make save spec.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m out \u001B[38;5;241m=\u001B[39m traced_model(example_input)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Using image_input in the inputs parameter:\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Convert to Core ML using the Unified Conversion API.\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m converted_model \u001B[38;5;241m=\u001B[39m \u001B[43mct\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraced_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mct\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensorType\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexample_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Save the converted model.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m converted_model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmobilenet.mlmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\converters\\_converters_entry.py:352\u001B[0m, in \u001B[0;36mconvert\u001B[1;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, useCPUOnly, package_dir, debug)\u001B[0m\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;241m!=\u001B[39m _MLPACKAGE_EXTENSION:\n\u001B[0;32m    350\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf package_dir is provided, it must have extension \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m (not \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(_MLPACKAGE_EXTENSION, ext))\n\u001B[1;32m--> 352\u001B[0m mlmodel \u001B[38;5;241m=\u001B[39m \u001B[43mmil_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_from\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexact_source\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexact_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclassifier_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclassifier_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtransforms\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_model_load\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_model_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_units\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_units\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpackage_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpackage_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exact_target \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmilinternal\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mlmodel \u001B[38;5;66;03m# Returns the MIL program\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\converters\\mil\\converter.py:183\u001B[0m, in \u001B[0;36mmil_convert\u001B[1;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;129m@_profile\u001B[39m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmil_convert\u001B[39m(\n\u001B[0;32m    146\u001B[0m     model,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    151\u001B[0m ):\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;124;03m    Convert model from a specified frontend `convert_from` to a specified\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;124;03m    converter backend `convert_to`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;124;03m        See `coremltools.converters.convert`\u001B[39;00m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_mil_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_from\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConverterRegistry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMLModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_units\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\converters\\mil\\converter.py:231\u001B[0m, in \u001B[0;36m_mil_convert\u001B[1;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001B[0m\n\u001B[0;32m    224\u001B[0m     package_path \u001B[38;5;241m=\u001B[39m _create_mlpackage(proto, weights_dir, kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpackage_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m modelClass(package_path,\n\u001B[0;32m    226\u001B[0m                       is_temp_package\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpackage_dir\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m    227\u001B[0m                       mil_program\u001B[38;5;241m=\u001B[39mmil_program,\n\u001B[0;32m    228\u001B[0m                       skip_model_load\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskip_model_load\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m    229\u001B[0m                       compute_units\u001B[38;5;241m=\u001B[39mcompute_units)\n\u001B[1;32m--> 231\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodelClass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mmil_program\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmil_program\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    233\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mskip_model_load\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mskip_model_load\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mcompute_units\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_units\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\models\\model.py:346\u001B[0m, in \u001B[0;36mMLModel.__init__\u001B[1;34m(self, model, useCPUOnly, is_temp_package, mil_program, skip_model_load, compute_units, weights_dir)\u001B[0m\n\u001B[0;32m    343\u001B[0m     filename \u001B[38;5;241m=\u001B[39m _tempfile\u001B[38;5;241m.\u001B[39mmktemp(suffix\u001B[38;5;241m=\u001B[39m_MLMODEL_EXTENSION)\n\u001B[0;32m    344\u001B[0m     _save_spec(model, filename)\n\u001B[1;32m--> 346\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__proxy__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spec, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_framework_error \u001B[38;5;241m=\u001B[39m \u001B[43m_get_proxy_and_spec\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_units\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_model_load\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_model_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    350\u001B[0m     _os\u001B[38;5;241m.\u001B[39mremove(filename)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\models\\model.py:123\u001B[0m, in \u001B[0;36m_get_proxy_and_spec\u001B[1;34m(filename, compute_units, skip_model_load)\u001B[0m\n\u001B[0;32m    120\u001B[0m     _MLModelProxy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    122\u001B[0m filename \u001B[38;5;241m=\u001B[39m _os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexpanduser(filename)\n\u001B[1;32m--> 123\u001B[0m specification \u001B[38;5;241m=\u001B[39m \u001B[43m_load_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _MLModelProxy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m skip_model_load:\n\u001B[0;32m    126\u001B[0m \n\u001B[0;32m    127\u001B[0m     \u001B[38;5;66;03m# check if the version is supported\u001B[39;00m\n\u001B[0;32m    128\u001B[0m     engine_version \u001B[38;5;241m=\u001B[39m _MLModelProxy\u001B[38;5;241m.\u001B[39mmaximum_supported_specification_version()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\coremltools\\models\\utils.py:210\u001B[0m, in \u001B[0;36mload_spec\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;124;03mLoad a protobuf model specification from file.\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03msave_spec\u001B[39;00m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _ModelPackage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[0;32m    211\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load libmodelpackage. Cannot make save spec.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    212\u001B[0m     )\n\u001B[0;32m    214\u001B[0m spec \u001B[38;5;241m=\u001B[39m _Model_pb2\u001B[38;5;241m.\u001B[39mModel()\n\u001B[0;32m    216\u001B[0m specfile \u001B[38;5;241m=\u001B[39m filename\n",
      "\u001B[1;31mException\u001B[0m: Unable to load libmodelpackage. Cannot make save spec."
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "# Trace the model with random data.\n",
    "\n",
    "example_input =  torch.rand(1, 3, 224, 224).to(device)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "out = traced_model(example_input)\n",
    "\n",
    "# Using image_input in the inputs parameter:\n",
    "# Convert to Core ML using the Unified Conversion API.\n",
    "converted_model = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.TensorType(shape=example_input.shape)]\n",
    " )\n",
    "\n",
    "# Save the converted model.\n",
    "converted_model.save(\"mobilenet.mlmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}